import os
import numpy as np
import tensorflow as tf
import cv2
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from IPython.display import Image, display
import config
from build_model import build_siamese_model

def find_target_layer(model):
    """
    Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„ØªÙØ§ÙÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø³ÙŠØ§Ù…ÙŠ - Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø©
    """
    print(f"[DEBUG] Searching for convolutional layers in model: {model.name}")
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù€ embedding network
    target_model = model
    
    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
    potential_layers = []
    
    # 1. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªÙØ§ÙÙŠØ© ÙÙŠ MobileNetV2
    for i, layer in enumerate(target_model.layers):
        layer_name = layer.name.lower()
        
        # Ø·Ø¨Ù‚Ø§Øª MobileNetV2 Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        if any(keyword in layer_name for keyword in ['conv', 'block', 'depthwise', 'expand']):
            try:
                if hasattr(layer, 'output_shape'):
                    shape = layer.output_shape
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø´ÙƒÙ„ Ø±Ø¨Ø§Ø¹ÙŠ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ (Batch, H, W, Channels)
                    if len(shape) == 4:
                        potential_layers.append(layer)
                        print(f"[DEBUG] Found potential layer {i}: {layer.name} - shape: {shape}")
            except:
                continue
    
    # 2. Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ØŒ Ù†Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø£ÙƒÙ…Ù„Ù‡
    if not potential_layers:
        for i, layer in enumerate(target_model.layers):
            try:
                if hasattr(layer, 'output_shape'):
                    shape = layer.output_shape
                    if len(shape) == 4:
                        potential_layers.append(layer)
                        print(f"[DEBUG] Found 4D layer {i}: {layer.name} - shape: {shape}")
            except:
                continue
    
    # 3. Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø·Ø¨Ù‚Ø©
    if not potential_layers:
        print("[DEBUG] Model layers summary:")
        for i, layer in enumerate(target_model.layers):
            print(f"  {i}: {layer.name} - {layer.__class__.__name__}")
            try:
                if hasattr(layer, 'output_shape'):
                    print(f"      shape: {layer.output_shape}")
            except:
                pass
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø¨Ù‚Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ù…Ø¹Ø±ÙˆÙØ© ÙÙŠ MobileNetV2
        known_layers = [
            'block_16_project_BN',  # Ø¢Ø®Ø± Ø·Ø¨Ù‚Ø© ÙÙŠ MobileNetV2
            'block_15_add',         # Ù‚Ø¨Ù„ Ø§Ù„Ø£Ø®ÙŠØ±Ø©
            'out_relu',             # Ø¥Ø®Ø±Ø§Ø¬ MobileNetV2
            'Conv_1',               # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
            'global_average_pooling2d'  # Ù‚Ø¯ ØªÙƒÙˆÙ† Ù‡Ø°Ù‡ Ù…ÙˆØ¬ÙˆØ¯Ø©
        ]
        
        for layer_name in known_layers:
            try:
                layer = target_model.get_layer(layer_name)
                potential_layers.append(layer)
                print(f"[DEBUG] Found known layer: {layer_name}")
                break
            except:
                continue
    
    if not potential_layers:
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø£ÙŠ Ø·Ø¨Ù‚Ø©ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø¢Ø®Ø± Ø·Ø¨Ù‚Ø© Ù‚Ø¨Ù„ Ø§Ù„Ù€ GlobalAveragePooling
        for i, layer in enumerate(target_model.layers):
            if 'global_average_pooling' not in layer.name.lower():
                last_non_pool_layer = layer
        
        if last_non_pool_layer:
            potential_layers.append(last_non_pool_layer)
            print(f"[DEBUG] Using last non-pooling layer: {last_non_pool_layer.name}")
    
    if potential_layers:
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø·Ø¨Ù‚Ø© Ù…Ù† Ù…Ù†ØªØµÙ Ø§Ù„Ø´Ø¨ÙƒØ©
        idx = len(potential_layers) // 2
        selected_layer = potential_layers[idx]
        print(f"[INFO] Selected layer: {selected_layer.name} (index {idx} of {len(potential_layers)})")
        return selected_layer.name
    else:
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø¢Ø®Ø± Ø·Ø¨Ù‚Ø©
        last_layer = target_model.layers[-1]
        print(f"[WARNING] Using last layer as fallback: {last_layer.name}")
        return last_layer.name

def make_siamese_heatmap(img_tensor, model, target_layer_name, reference_tensor=None):
    """
    Ø­Ø³Ø§Ø¨ Ø®Ø±ÙŠØ·Ø© Ø§Ù„ØªØ±ÙƒÙŠØ² Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø³ÙŠØ§Ù…ÙŠ
    """
    try:
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù€ embedding network
        try:
            embedding_network = model.get_layer('Embedding_Network')
        except:
            embedding_network = model
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙØ±Ø¹ÙŠ
        try:
            target_layer = embedding_network.get_layer(target_layer_name)
        except:
            print(f"[WARNING] Layer {target_layer_name} not found, using last layer")
            target_layer = embedding_network.layers[-1]
        
        grad_model = Model(
            inputs=embedding_network.input,
            outputs=[target_layer.output, embedding_network.output]
        )
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙˆØ±Ø© Ù…Ø±Ø¬Ø¹ÙŠØ© Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªÙˆÙÙŠØ±Ù‡Ø§
        if reference_tensor is None:
            reference_tensor = img_tensor
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª
        with tf.GradientTape() as tape:
            # ØªÙ…Ø±ÙŠØ± Ø§Ù„ØµÙˆØ±
            conv_output1, embedding1 = grad_model(img_tensor)
            conv_output2, embedding2 = grad_model(reference_tensor)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ (Cosine similarity)
            similarity = tf.reduce_sum(embedding1 * embedding2, axis=-1)
            target = tf.reduce_mean(similarity)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª
        grads = tape.gradient(target, conv_output1)
        
        if grads is None:
            print("[WARNING] Gradients are None, returning empty heatmap")
            # Ø¥Ø±Ø¬Ø§Ø¹ Ø®Ø±ÙŠØ·Ø© Ø¨Ø­Ø¬Ù… ØµØºÙŠØ± Ø§ÙØªØ±Ø§Ø¶ÙŠ
            return np.zeros((14, 14))
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ©
        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
        conv_output = conv_output1[0]
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
        if len(conv_output.shape) == 3 and len(pooled_grads.shape) == 1:
            heatmap = tf.reduce_sum(conv_output * pooled_grads, axis=-1)
        else:
            # Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
            heatmap = tf.reduce_mean(conv_output, axis=-1)
        
        # ØªØ·Ø¨ÙŠØ¹
        heatmap = tf.maximum(heatmap, 0)
        max_val = tf.math.reduce_max(heatmap)
        if max_val > 0:
            heatmap = heatmap / max_val
        
        heatmap_np = heatmap.numpy()
        
        # ØªØ³Ø¬ÙŠÙ„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ø®Ø±ÙŠØ·Ø©
        print(f"[DEBUG] Heatmap shape: {heatmap_np.shape}, min: {heatmap_np.min():.4f}, max: {heatmap_np.max():.4f}")
        
        return heatmap_np
        
    except Exception as e:
        print(f"[ERROR] Heatmap generation failed: {e}")
        # Ø¥Ø±Ø¬Ø§Ø¹ Ø®Ø±ÙŠØ·Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        return np.zeros((14, 14))

def prepare_image(img_path):
    """ØªØ­Ø¶ÙŠØ± ØµÙˆØ±Ø© Ù„Ù„Ø¥Ø¯Ø®Ø§Ù„ ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
    try:
        img = image.load_img(img_path, target_size=config.INPUT_SHAPE[:2])
        img_array = image.img_to_array(img)
        img_array = np.expand_dims(img_array, axis=0)
        img_array = preprocess_input(img_array)
        return tf.convert_to_tensor(img_array, dtype=tf.float32)
    except Exception as e:
        print(f"[ERROR] Failed to prepare image {img_path}: {e}")
        return None

def debug_model_structure():
    """ØªØµØ­ÙŠØ­ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
    print("\n" + "="*60)
    print("ØªØµØ­ÙŠØ­ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
    print("="*60)
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    siamese_model, embedding_network = build_siamese_model(config.INPUT_SHAPE)
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
    if os.path.exists(config.MODEL_PATH):
        embedding_network.load_weights(config.MODEL_PATH)
        print("[INFO] Model weights loaded")
    
    # Ø·Ø¨Ø§Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    print("\n[INFO] Embedding Network Structure:")
    print(f"Model name: {embedding_network.name}")
    print(f"Number of layers: {len(embedding_network.layers)}")
    
    print("\n[INFO] First 10 layers:")
    for i, layer in enumerate(embedding_network.layers[:10]):
        print(f"  {i}: {layer.name} - {layer.__class__.__name__}")
        try:
            if hasattr(layer, 'output_shape'):
                print(f"      Output shape: {layer.output_shape}")
        except:
            pass
    
    print("\n[INFO] Last 10 layers:")
    for i, layer in enumerate(embedding_network.layers[-10:]):
        idx = len(embedding_network.layers) - 10 + i
        print(f"  {idx}: {layer.name} - {layer.__class__.__name__}")
        try:
            if hasattr(layer, 'output_shape'):
                print(f"      Output shape: {layer.output_shape}")
        except:
            pass
    
    print("\n[INFO] Searching for convolutional layers...")
    conv_layers = []
    for i, layer in enumerate(embedding_network.layers):
        try:
            if hasattr(layer, 'output_shape'):
                shape = layer.output_shape
                if len(shape) == 4:  # 4D layers (convolutional)
                    conv_layers.append(layer)
                    print(f"  âœ“ {i}: {layer.name} - shape: {shape}")
        except:
            continue
    
    print(f"\n[INFO] Found {len(conv_layers)} convolutional layers")
    
    if conv_layers:
        # Ø§Ø®ØªÙŠØ§Ø± Ø·Ø¨Ù‚Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
        test_layer = conv_layers[len(conv_layers)//2]
        print(f"\n[INFO] Recommending layer: {test_layer.name}")
        return test_layer.name
    else:
        print("\n[ERROR] No convolutional layers found!")
        return None

def analyze_ameen_images():
    """ØªØ­Ù„ÙŠÙ„ ØµÙˆØ±ØªÙŠÙ† Ù„Ù„Ø´Ø®Øµ 'ameen' - Ù†Ø³Ø®Ø© Ù…Ø¨Ø³Ø·Ø©"""
    print("=" * 60)
    print("ØªØ­Ù„ÙŠÙ„ ØµÙˆØ±ØªÙŠÙ† Ù„Ù„Ø´Ø®Øµ: ameen")
    print("=" * 60)
    
    test_dir = config.TEST_DIR
    ameen_dir = os.path.join(test_dir, "ameen")
    
    if not os.path.exists(ameen_dir):
        print("[ERROR] Directory 'ameen' not found in test folder")
        return
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ ØµÙˆØ± ameen
    ameen_images = []
    for file in os.listdir(ameen_dir):
        if file.lower().endswith(('.jpg', '.jpeg', '.png')):
            ameen_images.append(os.path.join(ameen_dir, file))
    
    if len(ameen_images) < 2:
        print(f"[ERROR] Need at least 2 images for ameen, found {len(ameen_images)}")
        return
    
    print(f"[INFO] Found {len(ameen_images)} images for ameen")
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙˆÙ„ ØµÙˆØ±ØªÙŠÙ†
    img1_path = ameen_images[0]
    img2_path = ameen_images[1]
    
    print(f"[INFO] Image 1: {os.path.basename(img1_path)}")
    print(f"[INFO] Image 2: {os.path.basename(img2_path)}")
    
    # ØªØ­Ø¶ÙŠØ± Ø§Ù„ØµÙˆØ±
    print("\n[INFO] ØªØ­Ø¶ÙŠØ± Ø§Ù„ØµÙˆØ± Ù„Ù„Ù†Ù…ÙˆØ°Ø¬...")
    img1_tensor = prepare_image(img1_path)
    img2_tensor = prepare_image(img2_path)
    
    if img1_tensor is None or img2_tensor is None:
        print("[ERROR] Failed to prepare images")
        return
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    print("[INFO] ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø³ÙŠØ§Ù…ÙŠ...")
    siamese_model, embedding_network = build_siamese_model(config.INPUT_SHAPE)
    
    if os.path.exists(config.MODEL_PATH):
        embedding_network.load_weights(config.MODEL_PATH)
        print("[INFO] ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
    else:
        print("[ERROR] Ù…Ù„Ù Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        return
    
    try:
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡Ø§ Ù…Ù† Ø¯Ø§Ù„Ø© Ø§Ù„ØªØµØ­ÙŠØ­
        print("[INFO] Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©...")
        recommended_layer = debug_model_structure()
        
        if recommended_layer:
            target_layer = recommended_layer
        else:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¢Ø®Ø± Ø·Ø¨Ù‚Ø© ÙƒØ®ÙŠØ§Ø± Ø§Ø­ØªÙŠØ§Ø·ÙŠ
            target_layer = embedding_network.layers[-1].name
            print(f"[WARNING] Using last layer: {target_layer}")
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù…Ø¨Ø§Ø´Ø±Ø©
        print("[INFO] Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„ØµÙˆØ±ØªÙŠÙ†...")
        emb1 = embedding_network.predict(img1_tensor.numpy(), verbose=0)[0]
        emb2 = embedding_network.predict(img2_tensor.numpy(), verbose=0)[0]
        similarity = np.dot(emb1, emb2)
        
        print(f"[RESULT] Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {similarity:.4f}")
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø­Ø±Ø§Ø±ÙŠØ©
        try:
            print("[INFO] Ù…Ø­Ø§ÙˆÙ„Ø© Ø­Ø³Ø§Ø¨ Ø®Ø±ÙŠØ·Ø© Ø§Ù„ØªØ±ÙƒÙŠØ²...")
            heatmap1 = make_siamese_heatmap(img1_tensor, siamese_model, target_layer, img2_tensor)
            heatmap2 = make_siamese_heatmap(img2_tensor, siamese_model, target_layer, img1_tensor)
            heatmaps_available = True
        except Exception as e:
            print(f"[WARNING] Cannot generate heatmaps: {e}")
            heatmaps_available = False
            heatmap1 = np.zeros((14, 14))
            heatmap2 = np.zeros((14, 14))
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„ØµÙˆØ± Ù„Ù„Ø¹Ø±Ø¶
        img1_display = cv2.imread(img1_path)
        img1_display = cv2.resize(img1_display, (224, 224))
        img1_display_rgb = cv2.cvtColor(img1_display, cv2.COLOR_BGR2RGB)
        
        img2_display = cv2.imread(img2_path)
        img2_display = cv2.resize(img2_display, (224, 224))
        img2_display_rgb = cv2.cvtColor(img2_display, cv2.COLOR_BGR2RGB)
        
        # ØªÙØ³ÙŠØ± Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        if similarity > 0.7:
            similarity_text = "âœ… ØªØ´Ø§Ø¨Ù‡ Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹ (Ù†ÙØ³ Ø§Ù„Ø´Ø®Øµ Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯)"
            color = 'green'
        elif similarity > 0.5:
            similarity_text = "ğŸ‘ ØªØ´Ø§Ø¨Ù‡ Ø¬ÙŠØ¯ (Ù†ÙØ³ Ø§Ù„Ø´Ø®Øµ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ø¬Ø­)"
            color = 'blue'
        elif similarity > 0.3:
            similarity_text = "âš ï¸ ØªØ´Ø§Ø¨Ù‡ Ù…ØªÙˆØ³Ø· (Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù†ÙØ³ Ø§Ù„Ø´Ø®Øµ)"
            color = 'orange'
        else:
            similarity_text = "âŒ ØªØ´Ø§Ø¨Ù‡ Ø¶Ø¹ÙŠÙ (Ø´Ø®ØµÙŠÙ† Ù…Ø®ØªÙ„ÙÙŠÙ†)"
            color = 'red'
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        fig = plt.figure(figsize=(15, 8))
        
        # Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        gs = fig.add_gridspec(2, 3, height_ratios=[3, 1])
        
        # Ø§Ù„ØµÙˆØ±Ø© 1
        ax1 = fig.add_subplot(gs[0, 0])
        ax1.imshow(img1_display_rgb)
        ax1.set_title(f"Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰\n{os.path.basename(img1_path)}", fontsize=12)
        ax1.axis('off')
        
        # Ø§Ù„ØµÙˆØ±Ø© 2
        ax2 = fig.add_subplot(gs[0, 1])
        ax2.imshow(img2_display_rgb)
        ax2.set_title(f"Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©\n{os.path.basename(img2_path)}", fontsize=12)
        ax2.axis('off')
        
        # Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø­Ø±Ø§Ø±ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªØ§Ø­Ø©
        if heatmaps_available:
            # Ø®Ø±ÙŠØ·Ø© Ø§Ù„ØµÙˆØ±Ø© 1
            ax3 = fig.add_subplot(gs[0, 2])
            im3 = ax3.imshow(img1_display_rgb)
            overlay = ax3.imshow(heatmap1, cmap='jet', alpha=0.5)
            ax3.set_title("ØªØ±ÙƒÙŠØ² Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ - Ø§Ù„ØµÙˆØ±Ø© 1", fontsize=12)
            ax3.axis('off')
            
            # Ø®Ø±ÙŠØ·Ø© Ø§Ù„ØµÙˆØ±Ø© 2 ÙÙŠ ØµÙ Ø¬Ø¯ÙŠØ¯
            ax4 = fig.add_subplot(gs[1, 0])
            im4 = ax4.imshow(img2_display_rgb)
            ax4.imshow(heatmap2, cmap='jet', alpha=0.5)
            ax4.set_title("ØªØ±ÙƒÙŠØ² Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ - Ø§Ù„ØµÙˆØ±Ø© 2", fontsize=12)
            ax4.axis('off')
            
            # Ø´Ø±ÙŠØ· Ø§Ù„Ø£Ù„ÙˆØ§Ù†
            cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.3])
            plt.colorbar(overlay, cax=cbar_ax)
            cbar_ax.set_ylabel('Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø©', rotation=270, labelpad=15)
        
        # Ù…Ø±Ø¨Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        results_ax = fig.add_subplot(gs[1, 1:])
        results_ax.axis('off')
        
        results_text = (
            f"ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„:\n\n"
            f"ğŸ¯ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {similarity:.4f}\n"
            f"ğŸ“‹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: {similarity_text}\n\n"
            f"ğŸ” ØªÙØ³ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬:\n"
            f"â€¢ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ > 0.7: Ù†ÙØ³ Ø§Ù„Ø´Ø®Øµ Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯\n"
            f"â€¢ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ 0.5-0.7: Ù†ÙØ³ Ø§Ù„Ø´Ø®Øµ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ø¬Ø­\n"
            f"â€¢ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ 0.3-0.5: ÙŠØ­ØªØ§Ø¬ Ù…Ø²ÙŠØ¯Ø§Ù‹ Ù…Ù† Ø§Ù„ÙØ­Øµ\n"
            f"â€¢ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ < 0.3: Ø´Ø®ØµÙŠÙ† Ù…Ø®ØªÙ„ÙÙŠÙ†\n\n"
            f"ğŸ’¡ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙ‚Ù†ÙŠØ©:\n"
            f"â€¢ Ø§Ù„Ø¨Ø¹Ø¯ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ÙŠ: {emb1.shape[0]}\n"
            f"â€¢ Ù…Ø¹ÙŠØ§Ø± Ø§Ù„Ù…ØªØ¬Ù‡ 1: {np.linalg.norm(emb1):.4f}\n"
            f"â€¢ Ù…Ø¹ÙŠØ§Ø± Ø§Ù„Ù…ØªØ¬Ù‡ 2: {np.linalg.norm(emb2):.4f}"
        )
        
        results_ax.text(0.05, 0.95, results_text, 
                       fontsize=11, 
                       verticalalignment='top',
                       color=color if color != 'green' else 'darkgreen',
                       transform=results_ax.transAxes)
        
        plt.suptitle(f"ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¬Ù‡ - ameen", fontsize=16, y=0.98)
        plt.tight_layout()
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©
        output_dir = "output"
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, "ameen_face_analysis.png")
        plt.savefig(output_path, bbox_inches='tight', dpi=150, facecolor='white')
        plt.close()
        
        print(f"\nâœ… [SUCCESS] ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ: {output_path}")
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø©
        print("\n" + "="*60)
        print("Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„:")
        print("="*60)
        display(Image(filename=output_path))
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ù…Ø·ÙˆØ±
        print("\nğŸ“ˆ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©:")
        print(f"   - Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {similarity:.4f}")
        print(f"   - Ø§Ù„Ø²Ø§ÙˆÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ù…ØªØ¬Ù‡ÙŠÙ†: {np.degrees(np.arccos(np.clip(similarity, -1, 1))):.1f}Â°")
        print(f"   - Ø§Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ø¥Ù‚Ù„ÙŠØ¯ÙŠØ©: {np.linalg.norm(emb1 - emb2):.4f}")
        
        if similarity > 0.5:
            print("\nğŸ‰ Ø§Ù„Ø®Ù„Ø§ØµØ©: Ø§Ù„ØµÙˆØ±ØªØ§Ù† Ù„Ù†ÙØ³ Ø§Ù„Ø´Ø®Øµ (ameen)")
        else:
            print("\nâš ï¸ Ø§Ù„Ø®Ù„Ø§ØµØ©: Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„ØµÙˆØ±ØªØ§Ù† Ù„Ø´Ø®ØµÙŠÙ† Ù…Ø®ØªÙ„ÙÙŠÙ†")
            
    except Exception as e:
        print(f"[ERROR] ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")
        import traceback
        traceback.print_exc()

def main():
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    print("=" * 60)
    print("Ù†Ø¸Ø§Ù… ØªÙØ³ÙŠØ± Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø³ÙŠØ§Ù…ÙŠ")
    print("=" * 60)
    
    print("\nØ§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„:")
    print("1. ØªØ­Ù„ÙŠÙ„ ØµÙˆØ±ØªÙŠÙ† Ù„Ù„Ø´Ø®Øµ 'ameen'")
    print("2. ØªØµØ­ÙŠØ­ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ù„Ù„Ù…Ø·ÙˆØ±ÙŠÙ†)")
    print("3. Ø§Ù„Ø®Ø±ÙˆØ¬")
    
    try:
        choice = input("\nØ§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ù‚Ù… Ø§Ù„Ø®ÙŠØ§Ø± (1-3): ").strip()
        
        if choice == "1":
            analyze_ameen_images()
        elif choice == "2":
            debug_model_structure()
        elif choice == "3":
            print("[INFO] Ø§Ù„Ø®Ø±ÙˆØ¬...")
        else:
            print("[ERROR] Ø®ÙŠØ§Ø± ØºÙŠØ± ØµØ­ÙŠØ­")
            
    except KeyboardInterrupt:
        print("\n[INFO] Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø£Ù„ØºÙŠØª")
    except Exception as e:
        print(f"[ERROR] {e}")

if __name__ == "__main__":
    main()
